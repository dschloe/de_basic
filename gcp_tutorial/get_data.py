from google.cloud import bigquery
import os

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/Admin/Desktop/de_basic/gcp_tutorial/lgu6h-project-09e19171e4dd.json"

# BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = bigquery.Client()

# í…Œì´ë¸” ì§€ì •
project_id = "lgu6h-project"  # ì‹¤ì œ GCP í”„ë¡œì íŠ¸ IDë¡œ ë°”ê¾¸ê¸°
dataset_id = "sample_data"     # BigQuery ë°ì´í„°ì…‹ ì´ë¦„
table_id = "iris_from_local"         # ì—…ë¡œë“œí•  í…Œì´ë¸” ì´ë¦„

table_ref = f"{project_id}.{dataset_id}.{table_id}"
file_path = "C:/Users/Admin/Desktop/de_basic/gcp_tutorial/iris.csv"  # ì—…ë¡œë“œí•  ë¡œì»¬ íŒŒì¼ ê²½ë¡œ

# ì—…ë¡œë“œ ì„¤ì •
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,
    autodetect=True
)

# ë°ì´í„° ì—…ë¡œë“œ
with open(file_path, "rb") as source_file:
    load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)

print("ì—…ë¡œë“œ ì¤‘...")
load_job.result()  # ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
print("ì—…ë¡œë“œ ì™„ë£Œ!")

# ì—…ë¡œë“œ ê²°ê³¼ ì¶œë ¥
destination_table = client.get_table(table_ref)
print(f"ğŸ“Š {destination_table.num_rows} rows loaded to {table_ref}")
